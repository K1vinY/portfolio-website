<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI-based LLM Content Safety and Moderation Systems</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <h2>Kevin Yu</h2>
            </div>
            <ul class="nav-menu">
                <li class="nav-item"><a href="index.html#home" class="nav-link">Home</a></li>
                <li class="nav-item"><a href="index.html#about" class="nav-link">About Me</a></li>
                <li class="nav-item"><a href="index.html#portfolio" class="nav-link">Projects</a></li>
                <li class="nav-item"><a href="index.html#experience" class="nav-link">Experience</a></li>
                <li class="nav-item"><a href="index.html#publications" class="nav-link">Publications</a></li>
                <li class="nav-item"><a href="index.html#resume" class="nav-link">Resume</a></li>
                <li class="nav-item"><a href="index.html#contact" class="nav-link">Contact</a></li>
            </ul>
            <div class="hamburger">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </nav>

    <section class="project-detail">
        <div class="container">
            <div class="back-button">
                <a href="index.html#portfolio" class="back-link"><i class="fas fa-arrow-left"></i> Back to Projects</a>
            </div>

            <div class="project-header">
                <h1 class="project-title">AI-based LLM Content Safety and Moderation Systems</h1>
                <div class="project-meta">
                    <div class="meta-item"><i class="fas fa-calendar-alt"></i><span>Jul 2025 - Aug 2025</span></div>
                    <div class="meta-item"><i class="fas fa-map-marker-alt"></i><span>Institute for Information Industry (Summer Internship)</span></div>
                    <div class="meta-item"><i class="fas fa-tag"></i><span>AI Systems</span></div>
                </div>
            </div>

            <div class="project-summary">
                <h2>Abstract</h2>
                <p>As generative AI systems are increasingly integrated into real-world applications, they also face rising risks of prompt injection, malicious input, and harmful content generation. This project presents an end-to-end content safety detection system designed to proactively classify unsafe user prompts and LLM outputs in real time. Combining custom-trained models with a full-stack deployment architecture, the system aims to enhance security and trust in LLM-based tools by providing immediate, interpretable feedback to users and moderators.</p>
            </div>

            <div class="project-content">
                <div class="content-section">
                    <h2>The What</h2>
                    <div class="content-text">
                        <p>The rise of LLM applications across industries—from education to healthcare—has introduced new vectors for misuse, including prompt injection, sensitive data leakage, and toxic content generation. However, few reliable tools exist to detect and mitigate such threats in real-time. This project tackles that gap by building a scalable AI safety system that can classify both user inputs and LLM outputs for potential risk.</p>
                    </div>
                </div>

                <div class="content-section">
                    <h2>The How</h2>
                    <div class="content-text">
                        <p>I developed a full-stack system that combines a custom-trained multi-label classification model with a FastAPI backend and a React-based frontend. The model was designed to detect harmful behaviors such as prompt injection and toxic content in both user inputs and LLM outputs. Prediction results and safety scores were stored in a PostgreSQL database, and visualized through an interactive dashboard, allowing security analysts to quickly interpret risks and track abnormal usage patterns.</p>
                    </div>
                </div>

                <div class="content-section">
                    <h2>Results & Impact</h2>
                    <div class="results-content">
                        <div class="results-text">
                            <p>The system achieved 86% accuracy in detecting unsafe prompts and harmful model outputs during internal testing. The dashboard significantly reduced manual review time for security analysts and allowed for faster, more confident decisions during high-risk prompt evaluations. This tool lays the groundwork for responsible AI deployment by integrating safety detection directly into LLM workflows.</p>
                        </div>
                        <div class="results-images">
                            <img src="images/safety-result1.png" alt="LLM Safety Result 1">
                            <img src="images/safety-result2.png" alt="LLM Safety Result 2">
                        </div>
                    </div>
                </div>
            </div>

            <div class="project-technologies">
                <h3>Technologies Used</h3>
                <div class="tech-tags">
                    <span class="tech-tag">Python</span>
                    <span class="tech-tag">FastAPI</span>
                    <span class="tech-tag">React.js</span>
                    <span class="tech-tag">PostgreSQL</span>
                    <span class="tech-tag">HuggingFace</span>
                    <span class="tech-tag">Scikit-learn</span>
                </div>
            </div>

            <div class="project-links">
                <h3>Related Links</h3>
                <div class="link-buttons">
                    <a class="link-button" href="https://drive.google.com/file/d/1QjgLK07iKMaqX6m9bFiLKN5g44k55kJ8/view?usp=drive_link" target="_blank" rel="noopener noreferrer">Presentatioin</a>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Kevin Yu. All Rights Reserved.</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>


